{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9140cc-e6cf-4c0e-806f-6f4cd5d40a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an AI Agent to bakwards engineer contact information for varios public github repos that are shown to use certain dev tools and are qualified as potential sales targets.\n",
    "# This Agent also performs developer mapping and creates a profile based on a devs previously used toolset, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e136e8d-5bc8-41ff-9222-9b1020e68114",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get List of Public Repos that are relevant. Scrape GitHub, figure out rate limiting. Get list of every app using open ai. Add x to a vector db\n",
    "\n",
    "To switch search, do x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9c82980-8388-4d4d-bbea-0584d60b5613",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3560752671.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/91/bcl7bj6n2fqf9j03fdhyl02c0000gn/T/ipykernel_3405/3560752671.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    Public Repo - use LLM to get all contributors\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Public Repo:\n",
    "    \n",
    "How to tell if it's openAI's free tier, or would be paid:\n",
    "\n",
    "use LLM to get all contributors\n",
    "Analyze Name & Email in commits\n",
    "Check the email address in commit metadata, (if it's not anonymized by GitHub)\n",
    "username, name, bio, email, company, location, blog, followers, repos, contributions\n",
    "                                             \n",
    "Tools: git log or use GitHub's web interface to see commit history.\n",
    "Github Profile Bio\n",
    "                                             \n",
    "                                             \n",
    "Here is how chatGPT scraped Ashar's Identity: https://chatgpt.com/c/67e4a0ef-8d90-8008-8d3d-fafb4c1bd3a2                                                                         \n",
    "People often include their real name, location, personal website or even Linkedin. Scrape Bio, Find their name from that.                                         \n",
    "Combine GitHub name/location info with job titles to find likely LinkedIn profiles.\n",
    "\n",
    "Tools: LinkedIn Sales Navigator, PhantomBuster, or custom OpenAI-based fuzzy matche\n",
    "                                             \n",
    "                                             \n",
    "Reasoning:\n",
    "                                             \n",
    "                                             \n",
    "ü•à Semi-Reliable: Cross-Referencing\n",
    "Gravatar Lookup\n",
    "GitHub avatars come from Gravatar, which uses email hashes. You can sometimes reverse the hash if you have a suspected email.\n",
    "Handle Matching\n",
    "Look for the same handle across platforms (e.g., GitHub, LinkedIn, Twitter, Dev.to, Stack Overflow).\n",
    "Tools: Sherlock, Namechk\n",
    "Repository Context\n",
    "People sometimes name repos after their employers, universities, or personal projects (e.g., my-thesis-harvard).\n",
    "Contributors to Known Projects\n",
    "If they contribute to a company repo, check the org's team page.                                             \n",
    "For the Devs you can't get in touch with - make a PR                                             \n",
    "                                             \n",
    "                                             \n",
    "                                             \n",
    "private repo mapping\n",
    "\n",
    "                                             \n",
    "List of Companies you know are working on AI projects do Mapping project with every source you can find.\n",
    "\n",
    "\n",
    "Dev Mapping\n",
    "\n",
    "Most interesting repos are private. Need to figure out what tooling the devs have typically used, make an educated mapping profile on all of \n",
    "\n",
    "\n",
    "#Identify Repos that are using x foundational model over another foundational model. Do this for all foundational models, and then frameworks\n",
    "#Filter by commits, number of people, filter by whatever else to find projects that are actually relevant. \n",
    "\n",
    "\n",
    "\n",
    "                                             \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42363b35-cf93-4591-88ac-53840029acfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/kevincorstorphine/opt/anaconda3/lib/python3.9/site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "422387ab-e2c2-4d1b-8a70-a33d65cce2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "load_dotenv()\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "384f6ca6-3f48-480b-a2ed-f9291f74d09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Repo Name: OptimalScale/LMFlow\n",
      "üîó Repo URL:  https://github.com/OptimalScale/LMFlow\n",
      "Contributors https://api.github.com/repos/OptimalScale/LMFlow/contributors\n",
      "----------------------------------------\n",
      "üì¶ Repo Name: h2oai/h2ogpt\n",
      "üîó Repo URL:  https://github.com/h2oai/h2ogpt\n",
      "Contributors https://api.github.com/repos/h2oai/h2ogpt/contributors\n",
      "----------------------------------------\n",
      "üì¶ Repo Name: shlomota/MishnahBot\n",
      "üîó Repo URL:  https://github.com/shlomota/MishnahBot\n",
      "Contributors https://api.github.com/repos/shlomota/MishnahBot/contributors\n",
      "----------------------------------------\n",
      "üì¶ Repo Name: alfredcs/cavatar\n",
      "üîó Repo URL:  https://github.com/alfredcs/cavatar\n",
      "Contributors https://api.github.com/repos/alfredcs/cavatar/contributors\n",
      "----------------------------------------\n",
      "üì¶ Repo Name: filip-michalsky/SalesGPT\n",
      "üîó Repo URL:  https://github.com/filip-michalsky/SalesGPT\n",
      "Contributors https://api.github.com/repos/filip-michalsky/SalesGPT/contributors\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"Accept\": \"application/vnd.github+json\",\n",
    "    \"Authorization\": GITHUB_TOKEN\n",
    "}\n",
    "\n",
    "\n",
    "#MAKE LIST OF TOOLS YOU WANT TO BUILD A PROFILE FOR\n",
    "# if you want langchain OR claude, put OR in. If there is nothing, the space acts like an 'and'\n",
    "query = \"langchain claude chroma in:file language:python\"\n",
    "url = f\"https://api.github.com/search/code?q={query}&per_page=5&page=1\"\n",
    "              \n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Error:\", data.get(\"message\", \"Unknown error\"))\n",
    "    print(\"Status code:\", response.status_code)\n",
    "    print(\"Response headers:\", response.headers)\n",
    "else:\n",
    "   for item in data.get(\"items\", []):\n",
    "        repo = item[\"repository\"]\n",
    "        print(\"üì¶ Repo Name:\", repo[\"full_name\"])\n",
    "        print(\"üîó Repo URL: \", repo[\"html_url\"])\n",
    "        print(\"Contributors\", repo[\"contributors_url\"])\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\n",
    "#print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7725db00-b7e0-4c60-bdcd-3dd09742bd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Repo Name: OptimalScale/LMFlow\n",
      "üîó Repo URL: https://github.com/OptimalScale/LMFlow\n",
      "üë• Contributors:\n",
      "  - research4pan (440) ‚Äî https://github.com/research4pan\n",
      "  - wheresmyhair (298) ‚Äî https://github.com/wheresmyhair\n",
      "  - shizhediao (181) ‚Äî https://github.com/shizhediao\n",
      "  - hendrydong (176) ‚Äî https://github.com/hendrydong\n",
      "  - yaoguany (88) ‚Äî https://github.com/yaoguany\n",
      "--------------------------------------------------\n",
      "üì¶ Repo Name: h2oai/h2ogpt\n",
      "üîó Repo URL: https://github.com/h2oai/h2ogpt\n",
      "üë• Contributors:\n",
      "  - pseudotensor (6451) ‚Äî https://github.com/pseudotensor\n",
      "  - arnocandel (831) ‚Äî https://github.com/arnocandel\n",
      "  - achraf-mer (173) ‚Äî https://github.com/achraf-mer\n",
      "  - tloen (84) ‚Äî https://github.com/tloen\n",
      "  - fatihozturkh2o (60) ‚Äî https://github.com/fatihozturkh2o\n",
      "--------------------------------------------------\n",
      "üì¶ Repo Name: shlomota/MishnahBot\n",
      "üîó Repo URL: https://github.com/shlomota/MishnahBot\n",
      "üë• Contributors:\n",
      "  - shlomota (10) ‚Äî https://github.com/shlomota\n",
      "--------------------------------------------------\n",
      "üì¶ Repo Name: alfredcs/cavatar\n",
      "üîó Repo URL: https://github.com/alfredcs/cavatar\n",
      "üë• Contributors:\n",
      "  - alfredcs (118) ‚Äî https://github.com/alfredcs\n",
      "--------------------------------------------------\n",
      "üì¶ Repo Name: filip-michalsky/SalesGPT\n",
      "üîó Repo URL: https://github.com/filip-michalsky/SalesGPT\n",
      "üë• Contributors:\n",
      "  - filip-michalsky (173) ‚Äî https://github.com/filip-michalsky\n",
      "  - chemik-bit (157) ‚Äî https://github.com/chemik-bit\n",
      "  - iljamak (20) ‚Äî https://github.com/iljamak\n",
      "  - nickturner922 (8) ‚Äî https://github.com/nickturner922\n",
      "  - ishaan-jaff (7) ‚Äî https://github.com/ishaan-jaff\n",
      "--------------------------------------------------\n",
      "‚úÖ Saved contributor data to contributors.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"application/vnd.github+json\",\n",
    "    \"Authorization\": f\"{GITHUB_TOKEN}\"\n",
    "}\n",
    "\n",
    "query = \"langchain claude chroma in:file language:python\"\n",
    "url = f\"https://api.github.com/search/code?q={query}&per_page=5&page=1\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()\n",
    "\n",
    "contributors_data = []\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Error:\", data.get(\"message\", \"Unknown error\"))\n",
    "    print(\"Status code:\", response.status_code)\n",
    "else:\n",
    "    seen_repos = set()  # Avoid duplicate repos from code search\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        repo = item[\"repository\"]\n",
    "        full_name = repo[\"full_name\"]\n",
    "\n",
    "        if full_name in seen_repos:\n",
    "            continue\n",
    "        seen_repos.add(full_name)\n",
    "\n",
    "        repo_name = repo[\"full_name\"]\n",
    "        repo_url = repo[\"html_url\"]\n",
    "        contributors_url = repo[\"contributors_url\"]\n",
    "\n",
    "        print(\"üì¶ Repo Name:\", repo_name)\n",
    "        print(\"üîó Repo URL:\", repo_url)\n",
    "\n",
    "        contrib_response = requests.get(contributors_url, headers=headers)\n",
    "        if contrib_response.status_code == 200:\n",
    "            contributors = contrib_response.json()\n",
    "            contributor_list = [\n",
    "                {\n",
    "                    \"login\": c[\"login\"],\n",
    "                    \"contributions\": c[\"contributions\"],\n",
    "                    \"profile_url\": c[\"html_url\"]\n",
    "                }\n",
    "                for c in contributors[:5]  # limit to top 5\n",
    "            ]\n",
    "        else:\n",
    "            contributor_list = []\n",
    "            print(\"‚ùå Failed to fetch contributors:\", contrib_response.status_code)\n",
    "\n",
    "        contributors_data.append({\n",
    "            \"repo\": repo_name,\n",
    "            \"repo_url\": repo_url,\n",
    "            \"contributors\": contributor_list\n",
    "        })\n",
    "\n",
    "        print(\"üë• Contributors:\")\n",
    "        for c in contributor_list:\n",
    "            print(f\"  - {c['login']} ({c['contributions']}) ‚Äî {c['profile_url']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# ‚úÖ Now contributors_data is ready for LLM, CSV, etc.\n",
    "\n",
    "import csv\n",
    "\n",
    "csv_file = \"contributors.csv\"\n",
    "\n",
    "# Define the CSV column headers\n",
    "fieldnames = [\"repo\", \"repo_url\", \"contributor\", \"contributions\", \"profile_url\"]\n",
    "\n",
    "with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for entry in contributors_data:\n",
    "        for c in entry[\"contributors\"]:\n",
    "            writer.writerow({\n",
    "                \"repo\": entry[\"repo\"],\n",
    "                \"repo_url\": entry[\"repo_url\"],\n",
    "                \"contributor\": c[\"login\"],\n",
    "                \"contributions\": c[\"contributions\"],\n",
    "                \"profile_url\": c[\"profile_url\"]\n",
    "            })\n",
    "\n",
    "print(f\"‚úÖ Saved contributor data to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe4b45e4-4a00-41bb-b125-ad9d4bd42e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'repo': 'OptimalScale/LMFlow', 'repo_url': 'https://github.com/OptimalScale/LMFlow', 'contributors': [{'login': 'research4pan', 'contributions': 440, 'profile_url': 'https://github.com/research4pan'}, {'login': 'wheresmyhair', 'contributions': 298, 'profile_url': 'https://github.com/wheresmyhair'}, {'login': 'shizhediao', 'contributions': 181, 'profile_url': 'https://github.com/shizhediao'}, {'login': 'hendrydong', 'contributions': 176, 'profile_url': 'https://github.com/hendrydong'}, {'login': 'yaoguany', 'contributions': 88, 'profile_url': 'https://github.com/yaoguany'}]}, {'repo': 'h2oai/h2ogpt', 'repo_url': 'https://github.com/h2oai/h2ogpt', 'contributors': [{'login': 'pseudotensor', 'contributions': 6451, 'profile_url': 'https://github.com/pseudotensor'}, {'login': 'arnocandel', 'contributions': 831, 'profile_url': 'https://github.com/arnocandel'}, {'login': 'achraf-mer', 'contributions': 173, 'profile_url': 'https://github.com/achraf-mer'}, {'login': 'tloen', 'contributions': 84, 'profile_url': 'https://github.com/tloen'}, {'login': 'fatihozturkh2o', 'contributions': 60, 'profile_url': 'https://github.com/fatihozturkh2o'}]}, {'repo': 'shlomota/MishnahBot', 'repo_url': 'https://github.com/shlomota/MishnahBot', 'contributors': [{'login': 'shlomota', 'contributions': 10, 'profile_url': 'https://github.com/shlomota'}]}, {'repo': 'alfredcs/cavatar', 'repo_url': 'https://github.com/alfredcs/cavatar', 'contributors': [{'login': 'alfredcs', 'contributions': 118, 'profile_url': 'https://github.com/alfredcs'}]}, {'repo': 'filip-michalsky/SalesGPT', 'repo_url': 'https://github.com/filip-michalsky/SalesGPT', 'contributors': [{'login': 'filip-michalsky', 'contributions': 173, 'profile_url': 'https://github.com/filip-michalsky'}, {'login': 'chemik-bit', 'contributions': 157, 'profile_url': 'https://github.com/chemik-bit'}, {'login': 'iljamak', 'contributions': 20, 'profile_url': 'https://github.com/iljamak'}, {'login': 'nickturner922', 'contributions': 8, 'profile_url': 'https://github.com/nickturner922'}, {'login': 'ishaan-jaff', 'contributions': 7, 'profile_url': 'https://github.com/ishaan-jaff'}]}]\n"
     ]
    }
   ],
   "source": [
    "print(contributors_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55f7b14c-983a-422d-8b05-2f489f1ac79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Full contributor data saved to all_contributors.csv\n"
     ]
    }
   ],
   "source": [
    "contributor_data = []\n",
    "for entry in contributors_data:\n",
    "    repo = entry[\"repo\"]\n",
    "    repo_url = entry[\"repo_url\"]\n",
    "\n",
    "    for c in entry[\"contributors\"]:\n",
    "        # Fetch full user profile\n",
    "        user_resp = requests.get(f\"https://api.github.com/users/{c['login']}\", headers=headers)\n",
    "        if user_resp.status_code != 200:\n",
    "            print(f\"‚ö†Ô∏è Failed to get details for {c['login']}\")\n",
    "            continue\n",
    "        user_data = user_resp.json()\n",
    "\n",
    "        contributor_data.append({\n",
    "            \"repo\": repo,\n",
    "            \"repo_url\": repo_url,\n",
    "            \"login\": c[\"login\"],\n",
    "            \"contributions\": c[\"contributions\"],\n",
    "            \"profile_url\": c[\"profile_url\"],\n",
    "            \"name\": user_data.get(\"name\"),\n",
    "            \"company\": user_data.get(\"company\"),\n",
    "            \"blog\": user_data.get(\"blog\"),\n",
    "            \"location\": user_data.get(\"location\"),\n",
    "            \"email\": user_data.get(\"email\"),\n",
    "            \"bio\": user_data.get(\"bio\"),\n",
    "            \"twitter_username\": user_data.get(\"twitter_username\"),\n",
    "            \"followers\": user_data.get(\"followers\"),\n",
    "            \"public_repos\": user_data.get(\"public_repos\")\n",
    "        })\n",
    "\n",
    "        \n",
    "        import csv\n",
    "\n",
    "fields = [\n",
    "    \"repo\", \"repo_url\",\n",
    "    \"login\", \"contributions\", \"profile_url\",\n",
    "    \"name\", \"company\", \"blog\", \"location\", \"email\",\n",
    "    \"bio\", \"twitter_username\", \"followers\", \"public_repos\"\n",
    "]\n",
    "\n",
    "with open(\"all_contributors.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for row in contributor_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"‚úÖ Full contributor data saved to all_contributors.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956ec62-4eb0-4da5-99c7-d7b0d3284b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM Prompt:\n",
    "#is this code for business purposes? Is someone paying for this? In various scenarios, how much is this costing them. Realistically, why are they using the x tool, and why would x make sense?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3fc3a-ef8f-4bc3-8331-d5a3f0c936fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "add to a folder\n",
    "get requirements, dockerfile to better extract code?\n",
    "\n",
    "Get LLM to backfind contact info for each contributor and to make a profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "615b697c-8655-4380-9251-6e590dc74bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://chatgpt.com/c/67e2fa1a-2850-8008-99a8-bd908a6eb335\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf041d-76e3-49f5-a7cb-dd63584c9248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16cc0819-8047-4ffd-85d5-e8197c5fecdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character 'üîç' (U+1F50D) (3781455242.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/91/bcl7bj6n2fqf9j03fdhyl02c0000gn/T/ipykernel_21596/3781455242.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    üîç 1. Scrape and Analyze Public Repositories (e.g., GitHub)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character 'üîç' (U+1F50D)\n"
     ]
    }
   ],
   "source": [
    "# üîç 1. Scrape and Analyze Public Repositories (e.g., GitHub)\n",
    "# What to look for:\n",
    "\n",
    "# ML frameworks: TensorFlow, PyTorch, Hugging Face Transformers, etc.\n",
    "\n",
    "# Infrastructure: Docker, Kubernetes, Terraform\n",
    "\n",
    "# Languages: Python, Java, Go, etc.\n",
    "\n",
    "# Tooling: MLflow, Weights & Biases, Ray, etc.\n",
    "\n",
    "# How to do it:\n",
    "\n",
    "# Use GitHub‚Äôs API to pull repo metadata, requirements.txt, Dockerfile, etc.\n",
    "\n",
    "# Parse for keywords and dependencies.\n",
    "\n",
    "# Analyze commit history and contributors to see which teams or engineers worked on what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22020adf-72bd-4874-8299-21e10ca9eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† 2. Use Job Postings for Stack Clues\n",
    "# What to look for:\n",
    "\n",
    "# Job listings for ML Engineers, MLOps, or Applied Scientists.\n",
    "\n",
    "# Stacks and tools mentioned (they‚Äôre usually very specific).\n",
    "\n",
    "# Tools to help:\n",
    "\n",
    "# Diffbot or Apollo.io for scraping and structuring job post data.\n",
    "\n",
    "# Simple web scraping + LLM pipeline to extract and summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af461323-28f6-44b6-a7b2-659542284ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåê 3. Explore Company Engineering Blogs & Tech Talks\n",
    "# Places to check:\n",
    "\n",
    "# Medium, Dev.to, Substack, company tech blogs.\n",
    "\n",
    "# YouTube talks or conference slides (e.g., NeurIPS, PyData, MLConf).\n",
    "\n",
    "# How to analyze:\n",
    "\n",
    "# Use an agent (e.g. LangChain + LLM) to summarize or cross-reference stacks across companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863a3d1-9b3d-48cb-843b-f8921b1c925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß© 4. LinkedIn Engineering Profiles\n",
    "# Use case:\n",
    "\n",
    "# Search for engineers in ML roles at the target companies.\n",
    "\n",
    "# Analyze their LinkedIn summaries and endorsements for tooling or stack mentions.\n",
    "\n",
    "# Bonus: You can build a mini-graph of tools used across the org based on who lists what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed42043-365c-4c0d-a22a-e6b26bc44d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ 5. Inspect Their Products (Client/Server Side)\n",
    "# For web-based ML products:\n",
    "\n",
    "# Use browser dev tools or tools like Wappalyzer to detect front-end and API tools.\n",
    "\n",
    "# Look for WebAssembly, ONNX.js, TensorFlow.js, etc.\n",
    "\n",
    "# For APIs:\n",
    "\n",
    "# If their ML product exposes APIs, use Postman or curl to inspect endpoints and infer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cfb131-fb4e-458f-87ab-3a7baeb8eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ 6. Use Prebuilt Tools & Databases\n",
    "# Examples:\n",
    "\n",
    "# StackShare.io ‚Äì Check stack disclosures by company.\n",
    "\n",
    "# Crunchbase ‚Äì Look at ML acquisitions, product descriptions.\n",
    "\n",
    "# BuiltWith ‚Äì Web tech stack detection.\n",
    "\n",
    "# Papers With Code ‚Äì For more research-y orgs, see what papers are tied to which models and codebases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169914fb-7a8c-44ef-b2e3-0b9540d6c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† 7. Build an Agentic AI Researcher\n",
    "# If you're aiming for scale, consider building an agent that:\n",
    "\n",
    "# Takes in a list of companies.\n",
    "\n",
    "# Searches GitHub, job sites, blogs, LinkedIn.\n",
    "\n",
    "# Extracts stack mentions.\n",
    "\n",
    "# Aggregates and ranks the tools based on relevance or frequency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
